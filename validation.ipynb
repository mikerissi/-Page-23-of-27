{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "validation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4Xp9NhOCYSuz",
        "pECTKgw5ZvFK",
        "dER2_43pWj1T",
        "I-La4N9ObC1x",
        "yTghc_5HkJDW",
        "copyright-notice"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikerissi/-Page-23-of-27/blob/master/validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copyright-notice",
        "colab_type": "text"
      },
      "source": [
        "#### Copyright 2017 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "copyright-notice2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbIgBK-oXHO7",
        "colab_type": "text"
      },
      "source": [
        " # Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNX0VyBpHpCX",
        "colab_type": "text"
      },
      "source": [
        " **Objectifs d'apprentissage :**\n",
        "  * Utiliser plusieurs caractéristiques, plutôt qu'une seule, pour améliorer encore l'efficacité d'un modèle\n",
        "  * Déboguer les erreurs au niveau des données d'entrée du modèle\n",
        "  * Utiliser un ensemble de données d'évaluation pour déterminer si un modèle surapprend les données de validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za0m1T8CHpCY",
        "colab_type": "text"
      },
      "source": [
        " Comme pour les exercices précédents, vous allez utiliser l'ensemble de données sur l'immobilier en Californie pour tâcher de prédire la valeur médiane des logements (`median_house_value`) au niveau de l'îlot urbain, sur la base du recensement de 1990."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2zgMfWDWF12",
        "colab_type": "text"
      },
      "source": [
        " ## Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jErhkLzWI1B",
        "colab_type": "text"
      },
      "source": [
        " Commencez par charger et préparer les données. Cette fois, vous allez utiliser plusieurs caractéristiques. Vous allez donc modulariser la logique pour prétraiter un peu les caractéristiques :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwS5Bhm6HpCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format\n",
        "\n",
        "california_housing_dataframe = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n",
        "\n",
        "# california_housing_dataframe = california_housing_dataframe.reindex(\n",
        "#     np.random.permutation(california_housing_dataframe.index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2ZyTzX0HpCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_features(california_housing_dataframe):\n",
        "  \"\"\"Prepares input features from California housing data set.\n",
        "\n",
        "  Args:\n",
        "    california_housing_dataframe: A Pandas DataFrame expected to contain data\n",
        "      from the California housing data set.\n",
        "  Returns:\n",
        "    A DataFrame that contains the features to be used for the model, including\n",
        "    synthetic features.\n",
        "  \"\"\"\n",
        "  selected_features = california_housing_dataframe[\n",
        "    [\"latitude\",\n",
        "     \"longitude\",\n",
        "     \"housing_median_age\",\n",
        "     \"total_rooms\",\n",
        "     \"total_bedrooms\",\n",
        "     \"population\",\n",
        "     \"households\",\n",
        "     \"median_income\"]]\n",
        "  processed_features = selected_features.copy()\n",
        "  # Create a synthetic feature.\n",
        "  processed_features[\"rooms_per_person\"] = (\n",
        "    california_housing_dataframe[\"total_rooms\"] /\n",
        "    california_housing_dataframe[\"population\"])\n",
        "  return processed_features\n",
        "\n",
        "def preprocess_targets(california_housing_dataframe):\n",
        "  \"\"\"Prepares target features (i.e., labels) from California housing data set.\n",
        "\n",
        "  Args:\n",
        "    california_housing_dataframe: A Pandas DataFrame expected to contain data\n",
        "      from the California housing data set.\n",
        "  Returns:\n",
        "    A DataFrame that contains the target feature.\n",
        "  \"\"\"\n",
        "  output_targets = pd.DataFrame()\n",
        "  # Scale the target to be in units of thousands of dollars.\n",
        "  output_targets[\"median_house_value\"] = (\n",
        "    california_housing_dataframe[\"median_house_value\"] / 1000.0)\n",
        "  return output_targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZSIaDiaHpCf",
        "colab_type": "text"
      },
      "source": [
        " Pour l'**ensemble d'apprentissage**, vous allez choisir les 12 000 premiers exemples, sur un total de 17 000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9wejvw7HpCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_examples = preprocess_features(california_housing_dataframe.head(12000))\n",
        "training_examples.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlkgPR-SHpCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_targets = preprocess_targets(california_housing_dataframe.head(12000))\n",
        "training_targets.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l1aA2xOHpCj",
        "colab_type": "text"
      },
      "source": [
        " Pour l'**ensemble de validation**, vous allez choisir les 5 000 derniers exemples, sur un total de 17 000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLYXLWAiHpCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_examples = preprocess_features(california_housing_dataframe.tail(5000))\n",
        "validation_examples.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVPcIT3BHpCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_targets = preprocess_targets(california_housing_dataframe.tail(5000))\n",
        "validation_targets.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3TZV1pgfZ1n",
        "colab_type": "text"
      },
      "source": [
        " ## Tâche 1 : Examiner les données\n",
        "Examinons les données ci-dessus. On voit que `9` caractéristiques d'entrée peuvent être utilisées.\n",
        "\n",
        "Jetez un rapide coup d'œil sur le tableau des valeurs. Tout vous semble correct ? Voyez combien de problèmes vous pouvez relever. Ne vous inquiétez pas, aucune connaissance en statistiques n'est requise. Il suffit de faire preuve de bon sens.\n",
        "\n",
        "Après avoir eu l'occasion d'examiner les données par vous-même, consultez la solution. Vous y trouverez d'autres idées sur la manière de valider les données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xp9NhOCYSuz",
        "colab_type": "text"
      },
      "source": [
        " ### Solution\n",
        "\n",
        "Cliquez ci-dessous pour afficher la solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqeRmK57YWpy",
        "colab_type": "text"
      },
      "source": [
        " Voyons si les données sont conformes à quelques attentes élémentaires :\n",
        "\n",
        "* Pour certaines valeurs, telles que `median_house_value`, vous pouvez vérifier si les plages sont acceptables (pour rappel, ces données datent de 1990 !).\n",
        "\n",
        "* Pour d'autres valeurs, comme `latitude` et `longitude`, vous pouvez vérifier rapidement à l'aide d'une recherche Google si elles sont conformes aux résultats que l'on s'attend à obtenir.\n",
        "\n",
        "En y regardant de plus près, on peut constater quelques anomalies :\n",
        "\n",
        "* La valeur `median_income` se situe sur une échelle allant d'environ 3 à 15. Rien ne permet de déterminer avec certitude à quoi cette échelle fait référence : peut-être s'agit-il d'une échelle logarithmique ? On ne trouve aucune information à ce sujet ; on peut simplement supposer que les valeurs plus élevées correspondent à un revenu supérieur.\n",
        "\n",
        "* La valeur `median_house_value` maximale est de 500 001. Cela ressemble à une limite artificielle.\n",
        "\n",
        "* La caractéristique `rooms_per_person` se trouve généralement sur une échelle raisonnable, avec une valeur du 75e percentile correspondant approximativement à 2. Il existe toutefois quelques valeurs particulièrement grandes, comme 18 ou 55, qui peuvent indiquer une certaine corruption des données.\n",
        "\n",
        "Pour l'instant, vous utiliserez ces caractéristiques telles quelles. Cependant, on peut espérer que des exemples de ce type vous aident à comprendre comment vérifier les données qui vous arrivent en provenance d'une source inconnue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXliy7FYZZRm",
        "colab_type": "text"
      },
      "source": [
        " ## Tâche 2 : Représenter graphiquement la latitude/longitude par rapport à la valeur médiane des logements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJIWKBdfsDjg",
        "colab_type": "text"
      },
      "source": [
        " Examinons deux caractéristiques en particulier : **`latitude`** et **`longitude`**. Il s'agit des coordonnées géographiques de l'îlot urbain étudié.\n",
        "\n",
        "Cela peut produire une belle visualisation. Vous allez représenter graphiquement `latitude` et `longitude`, puis utiliser des couleurs pour afficher la valeur médiane des logements (`median_house_value`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_LD23bJ06TW",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(13, 8))\n",
        "\n",
        "ax = plt.subplot(1, 2, 1)\n",
        "ax.set_title(\"Validation Data\")\n",
        "\n",
        "ax.set_autoscaley_on(False)\n",
        "ax.set_ylim([32, 43])\n",
        "ax.set_autoscalex_on(False)\n",
        "ax.set_xlim([-126, -112])\n",
        "plt.scatter(validation_examples[\"longitude\"],\n",
        "            validation_examples[\"latitude\"],\n",
        "            cmap=\"coolwarm\",\n",
        "            c=validation_targets[\"median_house_value\"] / validation_targets[\"median_house_value\"].max())\n",
        "\n",
        "ax = plt.subplot(1,2,2)\n",
        "ax.set_title(\"Training Data\")\n",
        "\n",
        "ax.set_autoscaley_on(False)\n",
        "ax.set_ylim([32, 43])\n",
        "ax.set_autoscalex_on(False)\n",
        "ax.set_xlim([-126, -112])\n",
        "plt.scatter(training_examples[\"longitude\"],\n",
        "            training_examples[\"latitude\"],\n",
        "            cmap=\"coolwarm\",\n",
        "            c=training_targets[\"median_house_value\"] / training_targets[\"median_house_value\"].max())\n",
        "_ = plt.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32_DbjnfXJlC",
        "colab_type": "text"
      },
      "source": [
        " Une seconde ! Nous aurions dû avoir une belle carte de l’État de Californie, sur laquelle les villes les plus chères, comme San Francisco et Los Angeles, sont indiquées en rouge.\n",
        "\n",
        "C'est en quelque sorte ce que fait l'ensemble d'apprentissage, par rapport à une [carte réelle](https://www.google.com/maps/place/California/@37.1870174,-123.7642688,6z/data=!3m1!4b1!4m2!3m1!1s0x808fb9fe5f285e3d:0x8b5109a227086f55), mais ce n'est, de toute évidence, pas le cas de l'ensemble de validation.\n",
        "\n",
        "**Revenez en arrière et examinez à nouveau les données de la Tâche 1.**\n",
        "\n",
        "Voyez-vous d'autres différences dans les répartitions de caractéristiques ou cibles entre les données d'apprentissage et de validation ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pECTKgw5ZvFK",
        "colab_type": "text"
      },
      "source": [
        " ### Solution\n",
        "\n",
        "Cliquez ci-dessous pour afficher la solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49NC4_KIZxk_",
        "colab_type": "text"
      },
      "source": [
        " En observant les tableaux de statistiques récapitulatives ci-dessus, on est droit de se demander comment quelqu'un pourrait bien effectuer une vérification utile des données. Quelle est la valeur correcte du 75<sup>e</sup> percentile pour le nombre total de pièces (total_rooms) par îlot urbain ?\n",
        "\n",
        "Il est important de noter que, pour une caractéristique ou une colonne donnée, la répartition des valeurs entre les ensembles d'apprentissage et de validation devrait être sensiblement égale.\n",
        "\n",
        "Le fait que cela ne soit pas le cas est vraiment préoccupant, et indique une probable erreur dans la méthode utilisée pour créer les ensembles d'apprentissage et de validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "025Ky0Dq9ig0",
        "colab_type": "text"
      },
      "source": [
        " ## Tâche 3 : Revenir au code d'importation et de prétraitement des données, et détecter d'éventuels bugs\n",
        "Le cas échéant, corrigez le bug. Ne passez pas plus d'une ou deux minutes à rechercher le bug. Si vous ne le trouvez pas, vérifiez la solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFsd2eWHAMdy",
        "colab_type": "text"
      },
      "source": [
        " Après avoir détecté et corrigé l'erreur, exécutez à nouveau la cellule de traçage `latitude`/`longitude` ci-dessus et confirmez l'amélioration des évaluations d'intégrité.\n",
        "\n",
        "Au passage, signalons qu'il y a ici un enseignement important à tirer.\n",
        "\n",
        "**Dans le domaine du ML, le débogage s'apparente davantage à du *débogage de données* qu'à du débogage de code.**\n",
        "\n",
        "Si les données sont erronées, même le code ML le plus sophistiqué ne peut pas faire de miracles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dER2_43pWj1T",
        "colab_type": "text"
      },
      "source": [
        " ### Solution\n",
        "\n",
        "Cliquez ci-dessous pour afficher la solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnEVbYJvW2wu",
        "colab_type": "text"
      },
      "source": [
        " Voyons à présent comment les données sont rendues aléatoires lors de leur lecture.\n",
        "\n",
        "Si une application aléatoire des données n'est pas effectuée correctement avant de créer des ensembles d'apprentissage et de validation, des problèmes peuvent survenir en cas d'obtention de données classées ; ce qui semble être le cas ici."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCdqLpQyAos2",
        "colab_type": "text"
      },
      "source": [
        " ## Tâche 4 : Entraîner et évaluer un modèle\n",
        "\n",
        "**Consacrez environ cinq minutes à expérimenter différents hyperparamètres. Essayez d'obtenir les meilleures performances de validation possibles.**\n",
        "\n",
        "Vous allez ensuite entraîner une variable indépendante linéaire à l'aide de toutes les caractéristiques dans l'ensemble de données et voir quel est le résultat.\n",
        "\n",
        "Définissez la même fonction d'entrée que celle utilisée précédemment pour charger les données dans un modèle TensorFlow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzcIPGxxgG0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
        "    \"\"\"Trains a linear regression model of multiple features.\n",
        "  \n",
        "    Args:\n",
        "      features: pandas DataFrame of features\n",
        "      targets: pandas DataFrame of targets\n",
        "      batch_size: Size of batches to be passed to the model\n",
        "      shuffle: True or False. Whether to shuffle the data.\n",
        "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
        "    Returns:\n",
        "      Tuple of (features, labels) for next data batch\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convert pandas data into a dict of np arrays.\n",
        "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
        " \n",
        "    # Construct a dataset, and configure batching/repeating.\n",
        "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    \n",
        "    # Shuffle the data, if specified.\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(10000)\n",
        "    \n",
        "    # Return the next batch of data.\n",
        "    features, labels = ds.make_one_shot_iterator().get_next()\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvrKoBmNgRCO",
        "colab_type": "text"
      },
      "source": [
        " Étant donné que vous travaillez à présent avec plusieurs caractéristiques d'entrée, vous allez modulariser le code pour configurer des colonnes de caractéristiques dans une fonction distincte. Pour l'instant, ce code est relativement simple, car toutes les caractéristiques sont numériques. Cependant, nous allons le développer à mesure que nous utiliserons d'autres types de caractéristiques dans les prochains exercices.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEW5_XYtgZ-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_feature_columns(input_features):\n",
        "  \"\"\"Construct the TensorFlow Feature Columns.\n",
        "\n",
        "  Args:\n",
        "    input_features: The names of the numerical input features to use.\n",
        "  Returns:\n",
        "    A set of feature columns\n",
        "  \"\"\" \n",
        "  return set([tf.feature_column.numeric_column(my_feature)\n",
        "              for my_feature in input_features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0o2wnnzf8BD",
        "colab_type": "text"
      },
      "source": [
        " Vous allez maintenant exécuter le code `train_model()` ci-dessous pour configurer les fonctions d'entrée et calculer des prédictions.\n",
        "\n",
        "**REMARQUE :** Il est permis de faire référence au code des exercices précédents, mais veillez à appeler `predict()` sur les ensembles de données appropriés.\n",
        "\n",
        "Comparez les coûts sur les données d'apprentissage et de validation. Avec une seule caractéristique brute, la meilleure valeur RMSE (racine carrée de l'erreur quadratique moyenne) obtenue était d'environ 180.\n",
        "\n",
        "Voyons maintenant dans quelle mesure l'utilisation de plusieurs caractéristiques peut être bénéfique.\n",
        "\n",
        "Vérifiez les données en utilisant quelques-unes des méthodes mentionnées précédemment, à savoir :\n",
        "\n",
        "   * Comparaison des répartitions de prédictions et des valeurs cibles réelles\n",
        "\n",
        "   * Création d'un diagramme de dispersion des prédictions par rapport aux valeurs cibles\n",
        "\n",
        "   * Création de deux diagrammes de dispersion des données de validation à l'aide de `latitude` et `longitude` :\n",
        "      * Un diagramme associant une couleur à la valeur `median_house_value` cible réelle\n",
        "      * Un deuxième diagramme associant une couleur à la valeur `median_house_value` prédite pour un affichage comparatif."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXt0_4ZTEf4V",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def train_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a linear regression model of multiple features.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  as well as a plot of the training and validation loss over time.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    training_examples: A `DataFrame` containing one or more columns from\n",
        "      `california_housing_dataframe` to use as input features for training.\n",
        "    training_targets: A `DataFrame` containing exactly one column from\n",
        "      `california_housing_dataframe` to use as target for training.\n",
        "    validation_examples: A `DataFrame` containing one or more columns from\n",
        "      `california_housing_dataframe` to use as input features for validation.\n",
        "    validation_targets: A `DataFrame` containing exactly one column from\n",
        "      `california_housing_dataframe` to use as target for validation.\n",
        "      \n",
        "  Returns:\n",
        "    A `LinearRegressor` object trained on the training data.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "  steps_per_period = steps / periods\n",
        "  \n",
        "  # Create a linear regressor object.\n",
        "  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  linear_regressor = tf.estimator.LinearRegressor(\n",
        "      feature_columns=construct_feature_columns(training_examples),\n",
        "      optimizer=my_optimizer\n",
        "  )\n",
        "  \n",
        "  # 1. Create input functions.\n",
        "  training_input_fn = # YOUR CODE HERE\n",
        "  predict_training_input_fn = # YOUR CODE HERE\n",
        "  predict_validation_input_fn = # YOUR CODE HERE\n",
        "  \n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"RMSE (on training data):\")\n",
        "  training_rmse = []\n",
        "  validation_rmse = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    linear_regressor.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period,\n",
        "    )\n",
        "    # 2. Take a break and compute predictions.\n",
        "    training_predictions = # YOUR CODE HERE\n",
        "    validation_predictions = # YOUR CODE HERE\n",
        "    \n",
        "    # Compute training and validation loss.\n",
        "    training_root_mean_squared_error = math.sqrt(\n",
        "        metrics.mean_squared_error(training_predictions, training_targets))\n",
        "    validation_root_mean_squared_error = math.sqrt(\n",
        "        metrics.mean_squared_error(validation_predictions, validation_targets))\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, training_root_mean_squared_error))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_rmse.append(training_root_mean_squared_error)\n",
        "    validation_rmse.append(validation_root_mean_squared_error)\n",
        "  print(\"Model training finished.\")\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"RMSE\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
        "  plt.tight_layout()\n",
        "  plt.plot(training_rmse, label=\"training\")\n",
        "  plt.plot(validation_rmse, label=\"validation\")\n",
        "  plt.legend()\n",
        "\n",
        "  return linear_regressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFFRmvUGh8wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linear_regressor = train_model(\n",
        "    # TWEAK THESE VALUES TO SEE HOW MUCH YOU CAN IMPROVE THE RMSE\n",
        "    learning_rate=0.00001,\n",
        "    steps=100,\n",
        "    batch_size=1,\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-La4N9ObC1x",
        "colab_type": "text"
      },
      "source": [
        " ### Solution\n",
        "\n",
        "Cliquez ci-dessous pour afficher une solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyz6n1YHbGef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a linear regression model of multiple features.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  as well as a plot of the training and validation loss over time.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    training_examples: A `DataFrame` containing one or more columns from\n",
        "      `california_housing_dataframe` to use as input features for training.\n",
        "    training_targets: A `DataFrame` containing exactly one column from\n",
        "      `california_housing_dataframe` to use as target for training.\n",
        "    validation_examples: A `DataFrame` containing one or more columns from\n",
        "      `california_housing_dataframe` to use as input features for validation.\n",
        "    validation_targets: A `DataFrame` containing exactly one column from\n",
        "      `california_housing_dataframe` to use as target for validation.\n",
        "      \n",
        "  Returns:\n",
        "    A `LinearRegressor` object trained on the training data.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "  steps_per_period = steps / periods\n",
        "  \n",
        "  # Create a linear regressor object.\n",
        "  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  linear_regressor = tf.estimator.LinearRegressor(\n",
        "      feature_columns=construct_feature_columns(training_examples),\n",
        "      optimizer=my_optimizer\n",
        "  )\n",
        "  \n",
        "  # Create input functions.\n",
        "  training_input_fn = lambda: my_input_fn(\n",
        "      training_examples, \n",
        "      training_targets[\"median_house_value\"], \n",
        "      batch_size=batch_size)\n",
        "  predict_training_input_fn = lambda: my_input_fn(\n",
        "      training_examples, \n",
        "      training_targets[\"median_house_value\"], \n",
        "      num_epochs=1, \n",
        "      shuffle=False)\n",
        "  predict_validation_input_fn = lambda: my_input_fn(\n",
        "      validation_examples, validation_targets[\"median_house_value\"], \n",
        "      num_epochs=1, \n",
        "      shuffle=False)\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"RMSE (on training data):\")\n",
        "  training_rmse = []\n",
        "  validation_rmse = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    linear_regressor.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period,\n",
        "    )\n",
        "    # Take a break and compute predictions.\n",
        "    training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)\n",
        "    training_predictions = np.array([item['predictions'][0] for item in training_predictions])\n",
        "    \n",
        "    validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)\n",
        "    validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n",
        "    \n",
        "    \n",
        "    # Compute training and validation loss.\n",
        "    training_root_mean_squared_error = math.sqrt(\n",
        "        metrics.mean_squared_error(training_predictions, training_targets))\n",
        "    validation_root_mean_squared_error = math.sqrt(\n",
        "        metrics.mean_squared_error(validation_predictions, validation_targets))\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, training_root_mean_squared_error))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_rmse.append(training_root_mean_squared_error)\n",
        "    validation_rmse.append(validation_root_mean_squared_error)\n",
        "  print(\"Model training finished.\")\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"RMSE\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
        "  plt.tight_layout()\n",
        "  plt.plot(training_rmse, label=\"training\")\n",
        "  plt.plot(validation_rmse, label=\"validation\")\n",
        "  plt.legend()\n",
        "\n",
        "  return linear_regressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1imhjFzbWwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linear_regressor = train_model(\n",
        "    learning_rate=0.00003,\n",
        "    steps=500,\n",
        "    batch_size=5,\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65sin-E5NmHN",
        "colab_type": "text"
      },
      "source": [
        " ## Tâche 5 : Effectuer une évaluation sur les données de test\n",
        "\n",
        "**Dans la cellule ci-dessous, chargez l'ensemble de données de test et évaluez votre modèle sur celui-ci.**\n",
        "\n",
        "Les données de validation ayant fait l'objet de nombreuses itérations, assurez-vous qu'il n'y a pas eu de surapprentissage des particularités de cet échantillon.\n",
        "\n",
        "Les données de test se trouvent [ici](https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv).\n",
        "\n",
        "Comment vos performances de test se comparent-elles aux performances de validation ? Qu'est-ce que cela nous apprend sur les performances de généralisation de votre modèle ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icEJIl5Vp51r",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "california_housing_test_data = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\", sep=\",\")\n",
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTghc_5HkJDW",
        "colab_type": "text"
      },
      "source": [
        " ### Solution\n",
        "\n",
        "Cliquez ci-dessous pour afficher la solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xSYTarykO8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "california_housing_test_data = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\", sep=\",\")\n",
        "\n",
        "test_examples = preprocess_features(california_housing_test_data)\n",
        "test_targets = preprocess_targets(california_housing_test_data)\n",
        "\n",
        "predict_test_input_fn = lambda: my_input_fn(\n",
        "      test_examples, \n",
        "      test_targets[\"median_house_value\"], \n",
        "      num_epochs=1, \n",
        "      shuffle=False)\n",
        "\n",
        "test_predictions = linear_regressor.predict(input_fn=predict_test_input_fn)\n",
        "test_predictions = np.array([item['predictions'][0] for item in test_predictions])\n",
        "\n",
        "root_mean_squared_error = math.sqrt(\n",
        "    metrics.mean_squared_error(test_predictions, test_targets))\n",
        "\n",
        "print(\"Final RMSE (on test data): %0.2f\" % root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}